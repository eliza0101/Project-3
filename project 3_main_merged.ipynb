{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Input and Image Generation**\n",
    "\n",
    "Steps with Gradio:\n",
    "1. Ask User for Image Concepts: Use Gradio to create a text input interface where users can enter their image concepts.\n",
    "2. Clean User Input: Ensure the prompt is clean of unnecessary spaces or characters.\n",
    "3. Prompt the Image Generation Agent: Call APIs to generate images using Stable Diffusion XL and DALL-E based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "# For environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#For dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# For API calls\n",
    "import requests\n",
    "import openai\n",
    "\n",
    "# For image processing and viewing\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For UI\n",
    "import gradio as gr\n",
    "\n",
    "# For caption generation\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Load dependencies to measure text output\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# BLEU Score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Store the API key in a variable.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "STABILITY_AI_API_KEY = os.getenv(\"STABILITY_AI_API_KEY\")\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to view images for debugging\n",
    "def view_base64_image(base64_string):\n",
    "    \"\"\"Decodes a base64 encoded image and displays it using matplotlib.\"\"\"\n",
    "\n",
    "    # Decode the base64 string\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to clean user input by removing extra spaces\n",
    "def clean_user_input(user_query):\n",
    "    cleaned_query = ' '.join(user_query.split())\n",
    "    return cleaned_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Stability AI API and generate an image\n",
    "def generate_stability_ai_image(prompt):\n",
    "    host = 'https://api.stability.ai/v2beta/stable-image/generate/sd3'\n",
    "    params = {\n",
    "        \"prompt\" : prompt,\n",
    "        \"negative_prompt\" : '',\n",
    "        \"aspect_ratio\" : '1:1',\n",
    "        \"seed\" : 0,\n",
    "        \"output_format\" : 'jpeg',\n",
    "        \"model\" : \"sd3\",\n",
    "        \"mode\" : \"text-to-image\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"Accept\": \"image/*\",\n",
    "        \"Authorization\": f\"Bearer {STABILITY_AI_API_KEY}\"\n",
    "    }\n",
    "\n",
    "    # Encode parameters\n",
    "    files = {}\n",
    "    image = params.pop(\"image\", None)\n",
    "    mask = params.pop(\"mask\", None)\n",
    "    if image is not None and image != '':\n",
    "        files[\"image\"] = open(image, 'rb')\n",
    "    if mask is not None and mask != '':\n",
    "        files[\"mask\"] = open(mask, 'rb')\n",
    "    if len(files)==0:\n",
    "        files[\"none\"] = ''\n",
    "\n",
    "    # Send request\n",
    "    print(f\"Sending REST request to {host}...\")\n",
    "    response = requests.post(\n",
    "        host,\n",
    "        headers=headers,\n",
    "        files=files,\n",
    "        data=params\n",
    "    )\n",
    "    if not response.ok:\n",
    "        raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n",
    "\n",
    "    return base64.b64encode(response.content)\n",
    "    # To test the function: response = generate_stability_ai_image(\"cute shiba inu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Dall-E Open AI API and generate an image\n",
    "def call_dalle_api(prompt):\n",
    "    client = openai.OpenAI()\n",
    "    response = client.images.generate(\n",
    "    model=\"dall-e-2\",\n",
    "    prompt=prompt,\n",
    "    size=\"512x512\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    "    response_format=\"b64_json\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].b64_json\n",
    "    # To test the function: response = call_dalle_api(\"A realistic image of a shiba inu with a birthday hat on the street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert base64 string to a PIL Image object\n",
    "def base64_to_pil_image(base64_string):\n",
    "\n",
    "    # Decode the base64 string\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "\n",
    "    # Create a BytesIO object from the decoded data\n",
    "    image_bytes = BytesIO(image_data)\n",
    "\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(image_bytes)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# This hash function is to create a unique id for the prompt\n",
    "def generate_hash(input_string, algorithm='sha256'):\n",
    "    \"\"\"\n",
    "    Generate a hash for a given string using the specified algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        input_string (str): The input string to hash.\n",
    "        algorithm (str): The hashing algorithm to use. Options include 'md5', 'sha1', 'sha256', etc.\n",
    "        \n",
    "    Returns:\n",
    "        str: The hexadecimal representation of the hash.\n",
    "    \"\"\"\n",
    "    # Get the hashing function from hashlib based on the specified algorithm\n",
    "    hash_function = getattr(hashlib, algorithm)\n",
    "    \n",
    "    # Create a hash object and update it with the encoded string\n",
    "    hash_object = hash_function()\n",
    "    hash_object.update(input_string.encode('utf-8'))\n",
    "    \n",
    "    # Return the hexadecimal digest of the hash\n",
    "    return hash_object.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images and assess their quality\n",
    "def generate_images_with_quality(prompt):\n",
    "    unique_id = generate_hash(prompt, algorithm='md5')\n",
    "\n",
    "    cleaned_prompt = clean_user_input(prompt)\n",
    "    stability_ai_image = base64_to_pil_image(generate_stability_ai_image(cleaned_prompt))\n",
    "    dalle_image = base64_to_pil_image(call_dalle_api(cleaned_prompt))\n",
    "        \n",
    "    with open(f\"generated_images/{unique_id}_prompt.txt\", \"w\") as f:\n",
    "        # Write text to the file\n",
    "        f.write(f'{prompt}, {cleaned_prompt}')\n",
    "        \n",
    "    stability_ai_image.save(f'generated_images/{unique_id}_stability_ai.jpg')\n",
    "    dalle_image.save(f'generated_images/{unique_id}_dalle.jpg')\n",
    "\n",
    "    #sd_quality = assess_image_quality(sd_image)\n",
    "    #dalle_quality = assess_image_quality(dalle_image)\n",
    "    return stability_ai_image, dalle_image #, f\"Quality: {sd_quality}\", dalle_image, f\"Quality: {dalle_quality}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce output\n",
    "def create_image_record(prompt, user_preference):\n",
    "    unique_id = generate_hash(prompt, algorithm='md5')        \n",
    "    file_output = open('image_generation_results.csv', 'a')\n",
    "    file_output.write(f'\"{unique_id}\",\"{prompt}\",\"\",\"\",\"\",\"\",\"\",\"\",\"{user_preference}\"\\r\\n')\n",
    "    file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending REST request to https://api.stability.ai/v2beta/stable-image/generate/sd3...\n"
     ]
    }
   ],
   "source": [
    "def prefer_stability_ai(param):\n",
    "    create_image_record(param, 'stability_ai')\n",
    "\n",
    "def prefer_dalle(param):\n",
    "    create_image_record(param, 'dall-e')\n",
    "\n",
    "# Create the Gradio interface using Blocks\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Text-to-Image Generation\")\n",
    "    gr.Markdown(\"Input a prompt to generate images using Stability AI and Dall-E.\")\n",
    "\n",
    "    # Input component for text concept\n",
    "    concept_input = gr.Textbox(label=\"Enter Image Prompt\", placeholder=\"Describe the image...\")\n",
    "\n",
    "    # Create a submit button\n",
    "    generate_button = gr.Button(\"Generate Images\")\n",
    "\n",
    "    # Output components for displaying images\n",
    "    stability_output = gr.Image(type=\"pil\", label=\"Stability AI Image\")\n",
    "    dalle_output = gr.Image(type=\"pil\", label=\"Dall-E Image\")\n",
    "\n",
    "    stability_ai_button = gr.Button(\"I like Stability AI image\")\n",
    "    stability_ai_button.click(\n",
    "        fn=prefer_stability_ai, \n",
    "        inputs=[concept_input],\n",
    "        outputs=[]\n",
    "    )    \n",
    "\n",
    "    dalle_button = gr.Button(\"I like Dall-E image\")\n",
    "    dalle_button.click(\n",
    "        fn=prefer_dalle, \n",
    "        inputs=[concept_input],\n",
    "        outputs=[]\n",
    "    )    \n",
    "\n",
    "    # Button click event to generate images\n",
    "    generate_button.click(\n",
    "        fn=generate_images_with_quality, \n",
    "        inputs=concept_input, \n",
    "        outputs=[stability_output, dalle_output]\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Evaluation, Relevance Check, and Description Generation**\n",
    "\n",
    "Steps:\n",
    "1. Evaluate Image Relevance: Use an LLM to assess whether the generated image is relevant to the prompt.\n",
    "2. Generate Image Descriptions: Use a classifier to create descriptive summaries of the images.\n",
    "3. Compare Models: Analyze how each model's output aligns with the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for caption generation\n",
    "def generate_caption(image):\n",
    "    # Process the image and generate a caption\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_unique_id = '135fe23ef59c99cac8312212992f2000'\n",
    "\n",
    "# Load prompt for images\n",
    "prompt_file = open(f'generated_images/{current_unique_id}_prompt.txt')\n",
    "prompt = prompt_file.read().split(',')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate caption for Stability AI image\n",
    "img = Image.open(f\"generated_images/{current_unique_id}_stability_ai.jpg\")\n",
    "stability_ai_caption = generate_caption(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate caption for Dall-E image\n",
    "img = Image.open(f\"generated_images/{current_unique_id}_dalle.jpg\")\n",
    "dalle_caption = generate_caption(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.7598\n"
     ]
    }
   ],
   "source": [
    "# Generate Stability AI BLEU score\n",
    "# Prompted captions and generated captions\n",
    "reference_captions = [prompt]\n",
    "generated_captions = [stability_ai_caption]\n",
    "\n",
    "# Calculate BLEU scores for each pair of reference and generated captions\n",
    "bleu_scores = [sentence_bleu([ref.split()], gen.split()) for ref, gen in zip(reference_captions, generated_captions)]\n",
    "\n",
    "# Calculate the average BLEU score\n",
    "stability_ai_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print the average BLEU score\n",
    "print(f\"Average BLEU Score: {stability_ai_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Generate Dall-E BLEU score\n",
    "# Prompted captions and generated captions\n",
    "reference_captions = [prompt]\n",
    "generated_captions = [dalle_caption]\n",
    "\n",
    "# Calculate BLEU scores for each pair of reference and generated captions\n",
    "bleu_scores = [sentence_bleu([ref.split()], gen.split()) for ref, gen in zip(reference_captions, generated_captions)]\n",
    "\n",
    "# Calculate the average BLEU score\n",
    "dalle_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print the average BLEU score\n",
    "print(f\"Average BLEU Score: {dalle_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Rouge score\n",
    "def calculate_rouge(reference, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores for reference: ' a sea otter in a bathtub' and generated: 'a sea otter in a bath': {'rouge1': Score(precision=0.8333333333333334, recall=0.8333333333333334, fmeasure=0.8333333333333334), 'rouge2': Score(precision=0.8, recall=0.8, fmeasure=0.8000000000000002), 'rougeL': Score(precision=0.8333333333333334, recall=0.8333333333333334, fmeasure=0.8333333333333334)}\n"
     ]
    }
   ],
   "source": [
    "# Generate Stability AI ROUGE Score\n",
    "reference_captions = [prompt]\n",
    "generated_captions = [stability_ai_caption]\n",
    "for ref, gen in zip(reference_captions, generated_captions):\n",
    "    stability_ai_rouge_score = calculate_rouge(ref, gen)\n",
    "    print(f\"ROUGE scores for reference: '{ref}' and generated: '{gen}': {stability_ai_rouge_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores for reference: ' a sea otter in a bathtub' and generated: 'a sea otter in a bathtub': {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n"
     ]
    }
   ],
   "source": [
    "# Generate Dall-E ROUGE Score\n",
    "reference_captions = [prompt]\n",
    "generated_captions = [dalle_caption]\n",
    "for ref, gen in zip(reference_captions, generated_captions):\n",
    "    dalle_rouge_score = calculate_rouge(ref, gen)\n",
    "    print(f\"ROUGE scores for reference: '{ref}' and generated: '{gen}': {dalle_rouge_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tunji need to clean up graph, don't need to recalculate score (?)\n",
    "\n",
    "# Plot results\n",
    "# Sample reference and generated captions\n",
    "reference_captions = [prompt, prompt]\n",
    "generated_captions = [stability_ai_caption, dalle_caption]\n",
    "\n",
    "# Calculate BLEU scores\n",
    "bleu_scores = [sentence_bleu([ref.split()], gen.split()) for ref, gen in zip(reference_captions, generated_captions)]\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = [scorer.score(ref, gen) for ref, gen in zip(reference_captions, generated_captions)]\n",
    "rouge1_scores = [score['rouge1'].fmeasure for score in rouge_scores]\n",
    "rouge2_scores = [score['rouge2'].fmeasure for score in rouge_scores]\n",
    "rougeL_scores = [score['rougeL'].fmeasure for score in rouge_scores]\n",
    "\n",
    "# Prepare data for K-means\n",
    "X = np.array(list(zip(bleu_scores, rouge1_scores)))\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=1)  # Choose number of clusters\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('K-means Clustering of BLEU and ROUGE Scores')\n",
    "plt.xlabel('BLEU Score')\n",
    "plt.ylabel('ROUGE-1 Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "df = pd.read_csv('image_generation_results.csv')\n",
    "\n",
    "# Get record for prompt\n",
    "record = df[df[\"Unique Id\"] == current_unique_id]\n",
    "\n",
    "# Update fields\n",
    "record[\"Stability AI Caption\"] = stability_ai_caption\n",
    "record[\"Dall-E Caption\"] = dalle_caption\n",
    "record[\"Stability AI BLEU\"] = stability_ai_bleu\n",
    "record[\"Dall-E BLEU\"] = dalle_bleu\n",
    "record[\"Stability AI Rouge\"] = stability_ai_rouge_score\n",
    "record[\"Dall-E Rouge\"] = dalle_rouge_score\n",
    "df[df[\"Unique Id\"] == current_unique_id] = record\n",
    "\n",
    "# Save back to file\n",
    "df.to_csv('image_generation_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
