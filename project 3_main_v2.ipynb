{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Input and Image Generation**\n",
    "\n",
    "Steps with Gradio:\n",
    "1. Ask User for Image Concepts: Use Gradio to create a text input interface where users can enter their image concepts.\n",
    "2. Clean User Input: Ensure the prompt is clean of unnecessary spaces or characters.\n",
    "3. Prompt the Image Generation Agent: Call APIs to generate images using Stable Diffusion XL and DALL-E based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import requests\n",
    "import openai\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Store the API key in a variable.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "STABILITY_AI_API_KEY = os.getenv(\"STABILITY_AI_API_KEY\")\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to view images for debugging\n",
    "def view_base64_image(base64_string):\n",
    "    \"\"\"Decodes a base64 encoded image and displays it using matplotlib.\"\"\"\n",
    "\n",
    "    # Decode the base64 string\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to clean user input by removing extra spaces\n",
    "def clean_user_input(user_query):\n",
    "    cleaned_query = ' '.join(user_query.split())\n",
    "    return cleaned_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Stability AI API and generate an image\n",
    "def generate_stability_ai_image(prompt):\n",
    "    host = 'https://api.stability.ai/v2beta/stable-image/generate/sd3'\n",
    "    params = {\n",
    "        \"prompt\" : prompt,\n",
    "        \"negative_prompt\" : '',\n",
    "        \"aspect_ratio\" : '1:1',\n",
    "        \"seed\" : 0,\n",
    "        \"output_format\" : 'jpeg',\n",
    "        \"model\" : \"sd3\",\n",
    "        \"mode\" : \"text-to-image\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"Accept\": \"image/*\",\n",
    "        \"Authorization\": f\"Bearer {STABILITY_AI_API_KEY}\"\n",
    "    }\n",
    "\n",
    "    # Encode parameters\n",
    "    files = {}\n",
    "    image = params.pop(\"image\", None)\n",
    "    mask = params.pop(\"mask\", None)\n",
    "    if image is not None and image != '':\n",
    "        files[\"image\"] = open(image, 'rb')\n",
    "    if mask is not None and mask != '':\n",
    "        files[\"mask\"] = open(mask, 'rb')\n",
    "    if len(files)==0:\n",
    "        files[\"none\"] = ''\n",
    "\n",
    "    # Send request\n",
    "    print(f\"Sending REST request to {host}...\")\n",
    "    response = requests.post(\n",
    "        host,\n",
    "        headers=headers,\n",
    "        files=files,\n",
    "        data=params\n",
    "    )\n",
    "    if not response.ok:\n",
    "        raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n",
    "\n",
    "    return base64.b64encode(response.content)\n",
    "    # To test the function: response = generate_stability_ai_image(\"cute shiba inu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Dall-E Open AI API and generate an image\n",
    "def call_dalle_api(prompt):\n",
    "    client = openai.OpenAI()\n",
    "    response = client.images.generate(\n",
    "    model=\"dall-e-2\",\n",
    "    prompt=prompt,\n",
    "    size=\"512x512\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    "    response_format=\"b64_json\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].b64_json\n",
    "    # To test the function: response = call_dalle_api(\"A realistic image of a shiba inu with a birthday hat on the street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert base64 string to a PIL Image object\n",
    "def base64_to_pil_image(base64_string):\n",
    "\n",
    "    # Decode the base64 string\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "\n",
    "    # Create a BytesIO object from the decoded data\n",
    "    image_bytes = BytesIO(image_data)\n",
    "\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(image_bytes)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generate images and assess their quality\n",
    "def generate_images_with_quality(prompt):\n",
    "    cleaned_prompt = clean_user_input(prompt)\n",
    "    stability_ai_image = base64_to_pil_image(generate_stability_ai_image(cleaned_prompt))\n",
    "    dalle_image = base64_to_pil_image(call_dalle_api(cleaned_prompt))\n",
    "\n",
    "    timestamp = time.time()\n",
    "    \n",
    "    with open(f\"generated_images/{timestamp}_prompt.txt\", \"w\") as f:\n",
    "        # Write text to the file\n",
    "        f.write(f'{prompt}, {cleaned_prompt}')\n",
    "        \n",
    "    stability_ai_image.save(f'generated_images/{timestamp}_stability_ai.jpg')\n",
    "    dalle_image.save(f'generated_images/{timestamp}_dalle.jpg')\n",
    "\n",
    "    #sd_quality = assess_image_quality(sd_image)\n",
    "    #dalle_quality = assess_image_quality(dalle_image)\n",
    "    return stability_ai_image, dalle_image #, f\"Quality: {sd_quality}\", dalle_image, f\"Quality: {dalle_quality}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://9ec2ee4c85ff9c85cc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9ec2ee4c85ff9c85cc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending REST request to https://api.stability.ai/v2beta/stable-image/generate/sd3...\n"
     ]
    }
   ],
   "source": [
    "# Gradio interface setup for user interaction\n",
    "iface = gr.Interface(fn=generate_images_with_quality,\n",
    "                     inputs=\"text\",\n",
    "                     outputs=[gr.Image(type=\"pil\", label=\"Stability AI Image\"), gr.Image(type=\"pil\", label=\"Dall-E Image\")],\n",
    "                     title=\"Text-to-Image Generation\",\n",
    "                     description=\"Input a concept to generate images using Stability AI and Dall-E.\")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Preprocessing and Quality Assessment**\n",
    "\n",
    "Steps:\n",
    "1. Measure Image Quality: Preprocess images to evaluate their quality using a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image for quality assessment\n",
    "def preprocess_image(image):\n",
    "    gray_image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    resized_image = cv2.resize(gray_image, (256, 256))  # Resize image\n",
    "    normalized_image = resized_image / 255.0  # Normalize pixel values\n",
    "    return normalized_image.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with actual trained model for real use case\n",
    "image_quality_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the quality of an image using a classifier\n",
    "def assess_image_quality(image):\n",
    "    feature_vector = preprocess_image(image)\n",
    "    quality_score = image_quality_classifier.predict([feature_vector])[0]\n",
    "    return quality_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Evaluation, Relevance Check, and Description Generation**\n",
    "\n",
    "Steps:\n",
    "1. Evaluate Image Relevance: Use an LLM to assess whether the generated image is relevant to the prompt.\n",
    "2. Generate Image Descriptions: Use a classifier to create descriptive summaries of the images.\n",
    "3. Compare Models: Analyze how each model's output aligns with the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import openai\n",
    "\n",
    "# Imports for loading environment variables.\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the relevance of an image using LLM\n",
    "def evaluate_image_relevance(prompt, image):\n",
    "    evaluation_result = llm_evaluate(prompt, image)\n",
    "    relevance_score = compare_with_prompt(evaluation_result, prompt)\n",
    "    return relevance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LLM for evaluating relevance based on prompt and image data\n",
    "def llm_evaluate(prompt, image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",\n",
    "      prompt=f\"Evaluate the relevance of this image based on the prompt: {prompt}. Image data: {img_str}\",\n",
    "      max_tokens=50\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LLM evaluation result with original prompt\n",
    "def compare_with_prompt(evaluation_result, prompt):\n",
    "    return \"Relevant\" if \"relevant\" in evaluation_result.lower() else \"Not Relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate description\n",
    "def generate_description(image):\n",
    "    # Placeholder for a model that generates descriptions from images\n",
    "    description_model_output = \"A detailed description of the image.\"\n",
    "    return description_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Handling with Pandas**\n",
    "\n",
    "Steps:\n",
    "1. Store Results: Use Pandas to handle data and store results in a structured format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store results of generation and evaluation\n",
    "results_df = pd.DataFrame(columns=['Prompt', 'SD_Image_Quality', 'DALL_E_Image_Quality', 'SD_Relevance', 'DALL_E_Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>SD_Image_Quality</th>\n",
       "      <th>DALL_E_Image_Quality</th>\n",
       "      <th>SD_Relevance</th>\n",
       "      <th>DALL_E_Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A futuristic cityscape</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Prompt SD_Image_Quality DALL_E_Image_Quality SD_Relevance  \\\n",
       "0  A futuristic cityscape                8                    7     Relevant   \n",
       "\n",
       "  DALL_E_Relevance  \n",
       "0     Not Relevant  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulated values for debugging\n",
    "prompt = \"A futuristic cityscape\"\n",
    "sd_quality = 8\n",
    "dalle_quality = 7\n",
    "sd_relevance = \"Relevant\"\n",
    "dalle_relevance = \"Not Relevant\"\n",
    "\n",
    "# Adding data using pd.concat instead of append\n",
    "new_row = pd.DataFrame([{ \n",
    "    'Prompt': prompt, \n",
    "    'SD_Image_Quality': sd_quality, \n",
    "    'DALL_E_Image_Quality': dalle_quality, \n",
    "    'SD_Relevance': sd_relevance,\n",
    "    'DALL_E_Relevance': dalle_relevance\n",
    "}])\n",
    "\n",
    "results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending REST request to https://api.stability.ai/v2beta/stable-image/generate/sd3...\n"
     ]
    }
   ],
   "source": [
    "# Save results into CSV file for further analysis or reporting\n",
    "results_df.to_csv('image_generation_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
